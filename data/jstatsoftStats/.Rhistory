install.packages(c("BH", "broom", "devtools", "dplyr", "evaluate", "fields", "formatR", "geepack", "geosphere", "git2r", "highr", "httr", "jsonlite", "knitr", "lazyeval", "lme4", "MCMCpack", "mime", "miniCRAN", "nlme", "plyr", "psych", "quantreg", "RcppArmadillo", "rgdal", "rgeos", "rmarkdown", "rstudioapi", "rvest", "slam", "sp", "spatstat", "spdep", "survival", "tidyr", "useful", "VGAM", "WikidataR", "WikipediR", "withr", "XLConnect", "XLConnectJars", "xml2", "Zelig", "zoo"))
source("00-course-setup.r")
source("00-course-setup.r")
url <- "https://www.jstatsoft.org/about/editorialTeam"
url_parsed <- html_read(url)
source("00-course-setup.r")
wd <- getwd()
url <- "https://www.jstatsoft.org/about/editorialTeam"
url_parsed <- read_html(url)
names <- html_nodes(url_parsed, ".member a")
names
names <- html_nodes(url_parsed, ".member a") %>% html_text()
names
names2 <- html_nodes(url_parsed, "#group a") %>% html_text()
names2
affiliations <- html_nodes(url_parsed, "member li") %>% html_text()
affiliations <- html_nodes(url_parsed, ".member li") %>% html_text()
affiliations
str_detect(affiliations, "tatisti|athemati") %>% table
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
html_table(url)
url_parsed <- read_html(url)
html_table(url_parsed)
html_table(url_parsed, fill = TRUE)
tables <- html_table(url_parsed, fill = TRUE)
tables
tab <- tables[[4]]
tab <- tables[[2]]
tab
meetings <- tables[[2]]
meetings <- tables[[2]]
class(meetings)
table(meetings)
head(meetings)
table(meetings$Location)
table(meetings$Location) %>% sort()
?html_table
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
tables <- html_table(url_parsed)
tables <- html_table(url_parsed, fill = TRUE)
tables
tables[[1]]
tables[[2]]
buildings <- tables[[2]]
buildings <- tables[[5]]
buildings
tables[[7]]
buildings <- tables[[7]]
View(buildings)
table(buildings$`Country/region`) %>% sort
table(buildings$City) %>% sort
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
authors <- html_nodes(url_parsed, css = ".small-meta__item:nth-child(1) a") %>% html_text()
(1)
authors
table(authors) %>% sort
# set temporary working directory
tempwd <- ("data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
# construct list of urls
baseurl <- "http://www.jstatsoft.org/"
volurl <- paste0("v", seq(1,62,1))
volurl[1:9] <- paste0("v0", seq(1, 9, 1))
brurl <- paste0("b0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "/", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
names
urls_list
baseurl <- "http://www.jstatsoft.org/issue/view"
volurl <- paste0("v", seq(1,71,1))
volurl <- paste0("v", seq(1,71,1))
volurl
volurl <- paste0("v0", seq(1,71,1))
volurl[1:9] <- paste0("v00", seq(1, 9, 1))
volurl
brurl <- paste0("b0", seq(1,9,1))
urls_list <- paste0(baseurl, "i", volurl)
urls_list
# construct list of urls
baseurl <- "http://www.jstatsoft.org/issue/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,13,1))
urls_list <- paste0(baseurl, "i", volurl)
urls_list <- paste0(rep(urls_list, each = 9), "/", brurl)
urls_list
# construct list of urls
baseurl <- "http://www.jstatsoft.org/issue/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, "i", volurl)
urls_list <- paste0(rep(urls_list, each = 9), "/", brurl)
urls_list
# construct list of urls
baseurl <- "http://www.jstatsoft.org/issue/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
urls_list
## step 2: download pages
folder <- "htmls_reviews/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, "/", names[i]))
Sys.sleep(runif(1, 0, 1))
}
}
urls_list
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
urls_list
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
## step 2: download pages
folder <- "htmls_reviews/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, "/", names[i]))
Sys.sleep(runif(1, 0, 1))
}
}
listFiles <- list.files(folder, pattern = "v.*")
listFiles <- list.files(folder, pattern = "0.*")
folder <- "htmls_articles/"
folder <- "html_articles/"
url <- character()
folder <- "html_articles/"
listFiles <- list.files(folder, pattern = "0.*")
?list.files
listFilesPath <-  list.files(folder, pattern = "0.*", full.names = TRUE)
file.size(listFilesPath[1])
files_size <- sapply(listFilesPath, file.size)
length(listFiles)
files_size <- sapply(listFilesPath, file.size)
table(files_size)
listFiles[files_size == 22094]
list_iles <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
files_size <- sapply(list_files_path, file.size)
table(files_size)
delete_articles <- lsit_files[files_size == 22094]
delete_articles <- list_files[files_size == 22094]
delete_articles
delete_articles <- list_files_path[files_size == 22094]
delete_files <- list_files_path[files_size == 22094]
sapply(delete_files, file.remove)
html_out <- read_html(list_files_path[1])
html_out
table_out <- html_table(html_out)
table_out <- html_table(html_out, fill = TRUE)
table_out
table_out <- html_table(html_out, fill = TRUE)[[6]]
table_out
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
View(table_out)
t(table_out)
table_out_t <- t(table_out)
names(table_out_t)
table_out_t
class(table_out_t)
table_out
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datPublish <- character()
for (i in 1:length(listFiles)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("Page views:: [[::digit::]]+") %>% str_extract("[[::digit::]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract(pattern = "[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}$")
}
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(listFiles)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("Page views:: [[::digit::]]+") %>% str_extract("[[::digit::]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract(pattern = "[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}$")
}
i
list_files_path[603]
list_files_path[602]
length(list_files_path)
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("Page views:: [[::digit::]]+") %>% str_extract("[[::digit::]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract(pattern = "[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}$")
}
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish)
names(dat)
View(dat)
i=1
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
statistics
i=1
statistics[i] %>% str_extract("[[::digit::]]+") %>% as.numeric()
statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
statistics
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$")
datePublish
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace(".", "")
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace(".", "")
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
# construct data frame
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish)
head(dat)
dattop <- dat[order(dat$numViews, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numViews)
hist(dat$numDownload, breaks=30)
hist(dat$numViews, breaks=30)
plot(density(dat$numViews), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JSTATSOFT")
# set temporary working directory
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
